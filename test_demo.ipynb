{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from audioop import add\n",
    "import os\n",
    "import sys\n",
    "from numpy import random\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageDraw\n",
    "from tensorflow.python.framework.versions import VERSION as __version__\n",
    "import tensorflow as tf\n",
    "from imgaug import augmenters as iaa\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn import svm, datasets\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cambiamos el Directorio al propio de MASK_RCNN\n",
    "ROOT_DIR = './'\n",
    "#ROOT_DIR = '/home/saflex/Projecto_CleanSea/Mask_RCNN/Mask_RCNN-master'\n",
    "assert os.path.exists(ROOT_DIR), 'ROOT_DIR does not exist'\n",
    "\n",
    "# Import mrcnn libraries\n",
    "sys.path.append(ROOT_DIR)\n",
    "from mrcnn.config import Config\n",
    "from mrcnn import utils\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn import visualize\n",
    "from mrcnn.model import log\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "# tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directorio perteneciente a MASK-RCNN\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "#  Dataset\n",
    "############################################################\n",
    "class CleanSeaDataset(utils.Dataset):\n",
    "    def load_data(self, dataset_dir, subset):\n",
    "        # Train or validation dataset?\n",
    "        assert subset in [\"train_coco\", \"test_coco\"]\n",
    "        dataset_dir = os.path.join(dataset_dir, subset)\n",
    "        print(dataset_dir)\n",
    "\n",
    "        # Cargamos el archivo json\n",
    "        annotation_json = os.path.join(dataset_dir,\"annotations.json\")\n",
    "        json_file = open(annotation_json)\n",
    "        coco_json = json.load(json_file)\n",
    "        json_file.close()\n",
    "        print(\"\\nAnotaciones Cargadas\\n\")\n",
    "\n",
    "        # Añadimos los nombres de las clases usando el metodo de utils.Dataset\n",
    "        source_name = \"coco_like\"\n",
    "        for category in coco_json['categories']:\n",
    "            class_id = category['id']\n",
    "            class_name = category['name']\n",
    "            if class_id < 1:\n",
    "                print('Error: Class id for \"{}\" reserved for the background'.format(class_name))\n",
    "            else:\n",
    "                self.add_class(source_name, class_id, class_name)\n",
    "        print(\"Nombres Añadidos \\n\")\n",
    "\n",
    "        # Almacenamos las anotaciones\n",
    "        annotations = {}\n",
    "        for annotation in coco_json['annotations']:\n",
    "            image_id = annotation['image_id']\n",
    "            if image_id not in annotations:\n",
    "                annotations[image_id] = []\n",
    "            annotations[image_id].append(annotation)\n",
    "        print(\"Anotaciones Almacenadas\\n\")\n",
    "\n",
    "        # Almacenamos las imagenes y las añadimos al dataset\n",
    "        seen_images = {}\n",
    "        for image in coco_json['images']:\n",
    "            image_id = image['id']\n",
    "            if image_id in seen_images:\n",
    "                print(\"Warning: Skipping duplicate image id: {}\".format(image))\n",
    "            else:\n",
    "                seen_images[image_id] = image\n",
    "                try:\n",
    "                    image_file_name = image['file_name']\n",
    "                    image_width = image['width']\n",
    "                    image_height = image['height']\n",
    "                except KeyError as key:\n",
    "                    print(\"Warning: Skipping image (id: {}) with missing key: {}\".format(image_id, key))\n",
    "                \n",
    "                image_path = os.path.join(dataset_dir, image_file_name)\n",
    "                image_annotations = annotations[image_id]\n",
    "                \n",
    "                # Añadimos la imagen usando el metodo de utils.Dataset\n",
    "                self.add_image(\n",
    "                    source=source_name,\n",
    "                    image_id=image_id,\n",
    "                    path=image_path,\n",
    "                    width=image_width,\n",
    "                    height=image_height,\n",
    "                    annotations=image_annotations\n",
    "                )\n",
    "        print(\"Imagenes añadidas al Dataset\\n\")\n",
    "\n",
    "    def load_mask(self, image_id):\n",
    "        \"\"\" Carga la mascara de instancia para la imagen dada\n",
    "        MaskRCNN espera mascaras en forma de mapa de bits (altura, anchura e instancias)\n",
    "        Argumentos:\n",
    "            image_id: El ID de la imagen a la que vamos a cargar la mascara\n",
    "        Salida:\n",
    "            masks: Una cadena booleana con estructura (altura, anchya y la cuenta de instancias) con una mascara por instancia\n",
    "            class_ids: Una cadena de 1 dimension de clase ID de la instancia de la mascara \"\"\"\n",
    "        image_info = self.image_info[image_id]\n",
    "        annotations = image_info['annotations']\n",
    "        instance_masks = []\n",
    "        class_ids = []\n",
    "        \n",
    "        for annotation in annotations:\n",
    "            class_id = annotation['category_id']\n",
    "            mask = Image.new('1', (image_info['width'], image_info['height']))\n",
    "            mask_draw = ImageDraw.ImageDraw(mask, '1')\n",
    "            for segmentation in annotation['segmentation']:\n",
    "                mask_draw.polygon(segmentation, fill=1)\n",
    "                bool_array = np.array(mask) > 0\n",
    "                instance_masks.append(bool_array)\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "        mask = np.dstack(instance_masks)\n",
    "        class_ids = np.array(class_ids, dtype=np.int32)\n",
    "        return mask, class_ids\n",
    "\n",
    "    def image_reference(self, image_id):\n",
    "        \"\"\"Return the path of the image.\"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        if info[\"source\"] == \"object\":\n",
    "            return info[\"path\"]\n",
    "        else:\n",
    "            super(self.__class__, self).image_reference(image_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "#  Configuracion\n",
    "############################################################\n",
    "\n",
    "class CleanSeaConfig(Config):\n",
    "    \"\"\"\n",
    "    Configuracion para el entrenamiento con CleanSea Dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    # Nombre de la configuracion\n",
    "    NAME = \"debris\"\n",
    "\n",
    "    # We use a GPU with 12GB memory, which can fit two images.\n",
    "    # Adjust down if you use a smaller GPU.\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "    # Use small images for faster training. Set the limits of the small side\n",
    "    # the large side, and that determines the image shape.\n",
    "    IMAGE_MIN_DIM = 512\n",
    "    IMAGE_MAX_DIM = 512\n",
    "\n",
    "    # Numero de clases + el background\n",
    "    NUM_CLASSES = 1 + 19  # Cleansea tiene 19 clases\n",
    "\n",
    "    # Salta las detecciones con <50% de seguridad\n",
    "    DETECTION_MIN_CONFIDENCE = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "#  Evaluacion\n",
    "############################################################\n",
    "class InferenceConfig(CleanSeaConfig):\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "    USE_MINI_MASK = False\n",
    "\n",
    "inference_config = InferenceConfig()\n",
    "\n",
    "# Recreate the model in inference mode\n",
    "model = modellib.MaskRCNN(mode=\"inference\", \n",
    "                          config=inference_config,\n",
    "                          model_dir=MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights from  ./logs\\Mask_RCNN_Epoch-1000_Aug-severe_Size-50_Train-real_Fill-synth.h5\n"
     ]
    }
   ],
   "source": [
    "# Get path to saved weights\n",
    "# Either set a specific path or find last trained weights\n",
    "# model_path = os.path.join(MODEL_DIR, \"mask_rcnn_debris_weights1000DA5Heads.h5\")\n",
    "model_path = os.path.join(MODEL_DIR, \"Mask_RCNN_Epoch-1000_Aug-severe_Size-50_Train-real_Fill-synth.h5\")\n",
    "#model_path = model.find_last()\n",
    "\n",
    "# Load trained weights\n",
    "print(\"Loading weights from \", model_path)\n",
    "model.load_weights(model_path, by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuracion para dataset_test cargada\n",
      "\n",
      "D:\\Cleansea\\cleansea_dataset\\CocoFormatDataset\\test_coco\n",
      "\n",
      "Anotaciones Cargadas\n",
      "\n",
      "Error: Class id for \"background\" reserved for the background\n",
      "Nombres Añadidos \n",
      "\n",
      "Anotaciones Almacenadas\n",
      "\n",
      "Imagenes añadidas al Dataset\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Validation dataset\n",
    "dataset_test = CleanSeaDataset()\n",
    "print(\"Configuracion para dataset_test cargada\\n\")\n",
    "dataset_test.load_data(\"D:\\Cleansea\\cleansea_dataset\\CocoFormatDataset\", \"test_coco\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ax(rows=1, cols=1, size=8):\n",
    "    \"\"\"Return a Matplotlib Axes array to be used in\n",
    "    all visualizations in the notebook. Provide a\n",
    "    central point to control graph sizes.\n",
    "\n",
    "    Change the default size attribute to control the size\n",
    "    of rendered images\n",
    "    \"\"\"\n",
    "    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image 0 to process...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'CleanSeaDataset' object has no attribute 'num_classes'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13396\\4031054569.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0moriginal_image\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage_meta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgt_class_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgt_bbox\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgt_mask\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         modellib.load_image_gt(dataset_test, inference_config, \n\u001b[1;32m----> 9\u001b[1;33m                             image_id)\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"original_image\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_image\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Cleansea\\Mask_RCNN-Cleansea\\mrcnn\\model.py\u001b[0m in \u001b[0;36mload_image_gt\u001b[1;34m(dataset, config, image_id, augmentation)\u001b[0m\n\u001b[0;32m   1285\u001b[0m     \u001b[1;31m# Different datasets have different classes, so track the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1286\u001b[0m     \u001b[1;31m# classes supported in the dataset of this image.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1287\u001b[1;33m     \u001b[0mactive_class_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1288\u001b[0m     \u001b[0msource_class_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msource_class_ids\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mimage_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"source\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1289\u001b[0m     \u001b[0mactive_class_ids\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msource_class_ids\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'CleanSeaDataset' object has no attribute 'num_classes'"
     ]
    }
   ],
   "source": [
    "############################################################\n",
    "#  Deteccion Deseada vs Obtenida\n",
    "############################################################\n",
    "# Test on a random training image\n",
    "for image_id, path in enumerate(os.listdir(os.path.join(\"D:\\Cleansea\\cleansea_dataset\\CocoFormatDataset\", \"test_coco\",\"JPEGImages\"))):\n",
    "    print(f\"Image {image_id} to process...\")\n",
    "    original_image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "        modellib.load_image_gt(dataset_test, inference_config, \n",
    "                            image_id)\n",
    "\n",
    "    log(\"original_image\", original_image)\n",
    "    log(\"image_meta\", image_meta)\n",
    "    log(\"gt_class_id\", gt_class_id)\n",
    "    log(\"gt_bbox\", gt_bbox)\n",
    "    log(\"gt_mask\", gt_mask)\n",
    "    visualize.display_instances(original_image, gt_bbox, gt_mask, gt_class_id, \n",
    "                                dataset_test.class_names, figsize=(8, 8))\n",
    "    plt.show()\n",
    "    # Resultados de la deteccion procesada por el modelo\n",
    "    print(\"Detection done by trained model...\")\n",
    "    results = model.detect([original_image], verbose=1)\n",
    "    r = results[0]\n",
    "    visualize.display_instances(original_image, r['rois'], r['masks'], r['class_ids'], \n",
    "                                dataset_test.class_names, r['scores'], ax=get_ax(),figsize=(8,8))\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.11 ('cleansea')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cea2676ec9d0180d7a0a4cfbef118e05e2dcfabc73690db4cf2084bcf6d0071c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
